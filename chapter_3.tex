%   Filename    : chapter_4.tex 
\chapter{Research Methodology}
This chapter lists and discusses the specific steps and activities that will be performed  to accomplish the project. 
The discussion covers the activities from pre-proposal to Final SP Writing.

\section{Research Activities}
\subsection{Creation of the dataset} 
A dataset of sentences containing Generation Alpha slangs and its formal translation or an approximation of will be created. This will involve data scraping, use of existing datasets, or any other suitable methods of obtaining data. It will serve as both the training and testing dataset for the fine-tuning of the LLM.

\subsection{Identification of potential LLM to be used.} 
A report on potential LLMs to use for this study will be created using existing studies about LoRA finetuning and slang translation. This report will include each LLMs strengths and weaknesses as well as existing studies supporting each evidence.

\subsection{Lookup on available GPU on demand services} 
A research on available GPU rental services will be done to obtain the necessary computing power to conduct the LLM finetuning. These services will be compared with each other to obtain the service fitting for this study.

\subsection{Study on LoRA implementation for LLM}
LoRA implementation on LLMs will be studied upon. It will require reading various guides, primarily one created by HuggingFace as they are one of the largest repositories for prebuilt LLMs. They also have several in-depth guides on fine-tuning models for specific purposes..

\subsection{Preprocessing of data} 
The dataset will be verified and cleaned before use for the fine-tuning of the model. It is to ensure that all sentences contain at least one slang and their formal translations are grammatically and semantically correct. As LoRA does not tamper with existing knowledge of the model (Hu et al. 2021), we are free to focus on teaching the model the slang while leveraging its original knowledge to provide proper sentences. In addition, after cleaning up the dataset, it will be split into a training and testing set. A dataset for fine-tuning is ready by the end.

\subsection{Prototype implementation of LoRA}
The implementation of LoRA on the selected model will require a prototype implementation to make the full implementation easier and simpler. An option to use qLoRA for the smaller memory requirements in exchange of longer runtime (Raschka, 2023) can be used instead to allow the use of lower end hardware for this study. This prototype will serve as the foundation for the complete implementation of the algorithm and thus, requires it to use the selected computing service to prevent future alterations to adjust to the platform. 

\subsection{Implementation of LoRA on selected model}
A full implementation of LoRA will be done using the previously created prototype as a basis. Since it has been proven to work, this step will mostly involve fine-tuning the selected model and fixing any hidden bugs.

\subsection{Implementation on LLM Evaluation Metrics}
Evaluation metrics will be implemented to compare the base model with the fine-tuned one. These metrics will be used to determine if the fine-tuned model will perform better than the base model.

\subsection{Testing and Analysis of Results}
The fine-tuned model will be tested using the testing set of the dataset and will use the evaluation metrics to determine its performance. This would include descriptive information regarding the model and comparison with the original model.

\subsection{Documentation}
All members are tasked to provide accurate and detailed logs of their activities. It will serve both as documentation and as a progress tracker to determine how far the project is from being done. It will be done every week at the memberâ€™s leisure.


\section{Calendar of Activities}

	Table \ref{tab:timetableactivities} shows a Gantt chart of the activities.  Each bullet represents approximately
	one week worth of activity.
	
	%
	%  the following commands will be used for filling up the bullets in the Gantt chart
	%
	\newcommand{\weekone}{\textbullet}
	\newcommand{\weektwo}{\textbullet \textbullet}
	\newcommand{\weekthree}{\textbullet \textbullet \textbullet}
	\newcommand{\weekfour}{\textbullet \textbullet \textbullet \textbullet}
	
	
	\begin{table}[ht]  
		\centering
		\caption{Timetable of Activities} \vspace{0.25em}
		\begin{tabular}{|p{2in}|c|c|c|c|c|c|c|c|} \hline
			\centering Activities (2024-2025) 
			& Nov & Dec & Jan & Feb & Mar & Apr & May \\ \hline
			
			Creation of the dataset      
			&\weekone~~~ & & & & & &  \\ \hline
			
			Identification of potential LLM to be used 
			&\weekone~~~ & & & &  &  &  \\ \hline
			
			Lookup on available GPU on demand services     
			&\weekone~~~ & & & &  & &   \\ \hline
			
			Study on LoRA implementation for LLM     
			& ~\weekone & & & &  &  &  \\ \hline
			
			Preprocessing of data      
			& ~\weekthree & & & &  & &  \\ \hline
			
			Prototype implementation of LoRA 
			&~~~\weekone & \weekfour & & &  & &   \\ \hline
			
			Implementation of LoRA on selected model 
			& & &\weektwo~~ & &  &  &  \\ \hline
			
			Implementation on LLM Evaluation Metrics 
			& & &\weektwo  & &  &  &  \\ \hline
			
			Testing and Analysis of Results 
			& & & & \weekfour & &  &   \\ \hline
			
			Documentation 
			& ~~\weektwo  & \weekfour & \weekfour & \weekfour & \weekfour & &  \\ \hline
			
		\end{tabular}
		\label{tab:timetableactivities}
	\end{table}